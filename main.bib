@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})





@article{ershov2022trackmate,
  title={TrackMate 7: integrating state-of-the-art segmentation algorithms into tracking pipelines},
  author={Ershov, Dmitry and Phan, Minh-Son and Pylv{\"a}n{\"a}inen, Joanna W and Rigaud, St{\'e}phane U and Le Blanc, Laure and Charles-Orszag, Arthur and Conway, James RW and Laine, Romain F and Roy, Nathan H and Bonazzi, Daria and others},
  journal={Nature methods},
  volume={19},
  number={7},
  pages={829--832},
  year={2022},
  publisher={Nature Publishing Group US New York}
}


@article{bragantini2024ultrack,
  title={Ultrack: pushing the limits of cell tracking across biological scales},
  author={Bragantini, Jord{\~a}o and Theodoro, Ilan and Zhao, Xiang and Huijben, Teun APM and Hirata-Miyasaki, Eduardo and VijayKumar, Shruthi and Balasubramanian, Akilandeswari and Lao, Tiger and Agrawal, Richa and Xiao, Sheng and others},
  journal={bioRxiv},
  year={2024}
}

@article{TissueNet,
    doi = {10.1371/journal.pcbi.1005177},
    author = {Van Valen, David A. AND Kudo, Takamasa AND Lane, Keara M. AND Macklin, Derek N. AND Quach, Nicolas T. AND DeFelice, Mialy M. AND Maayan, Inbal AND Tanouchi, Yu AND Ashley, Euan A. AND Covert, Markus W.},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Deep Learning Automates the Quantitative Analysis of Individual Cells in Live-Cell Imaging Experiments},
    year = {2016},
    month = {11},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pcbi.1005177},
    pages = {1-24},
    abstract = {Live-cell imaging has opened an exciting window into the role cellular heterogeneity plays in dynamic, living systems. A major critical challenge for this class of experiments is the problem of image segmentation, or determining which parts of a microscope image correspond to which individual cells. Current approaches require many hours of manual curation and depend on approaches that are difficult to share between labs. They are also unable to robustly segment the cytoplasms of mammalian cells. Here, we show that deep convolutional neural networks, a supervised machine learning method, can solve this challenge for multiple cell types across the domains of life. We demonstrate that this approach can robustly segment fluorescent images of cell nuclei as well as phase images of the cytoplasms of individual bacterial and mammalian cells from phase contrast images without the need for a fluorescent cytoplasmic marker. These networks also enable the simultaneous segmentation and identification of different mammalian cell types grown in co-culture. A quantitative comparison with prior methods demonstrates that convolutional neural networks have improved accuracy and lead to a significant reduction in curation time. We relay our experience in designing and optimizing deep convolutional neural networks for this task and outline several design rules that we found led to robust performance. We conclude that deep convolutional neural networks are an accurate method that require less curation time, are generalizable to a multiplicity of cell types, from bacteria to mammalian cells, and expand live-cell imaging capabilities to include multi-cell type systems.},
    number = {11},

}

@ARTICLE{TN2,
  title     = "Whole-cell segmentation of tissue images with human-level
               performance using large-scale data annotation and deep learning",
  author    = "Greenwald, Noah F and Miller, Geneva and Moen, Erick and Kong,
               Alex and Kagel, Adam and Dougherty, Thomas and Fullaway,
               Christine Camacho and McIntosh, Brianna J and Leow, Ke Xuan and
               Schwartz, Morgan Sarah and Pavelchek, Cole and Cui, Sunny and
               Camplisson, Isabella and Bar-Tal, Omer and Singh, Jaiveer and
               Fong, Mara and Chaudhry, Gautam and Abraham, Zion and Moseley,
               Jackson and Warshawsky, Shiri and Soon, Erin and Greenbaum,
               Shirley and Risom, Tyler and Hollmann, Travis and Bendall, Sean C
               and Keren, Leeat and Graf, William and Angelo, Michael and Van
               Valen, David",
  journal   = "Nat. Biotechnol.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  40,
  number    =  4,
  pages     = "555--565",
  abstract  = "A principal challenge in the analysis of tissue imaging data is
               cell segmentation-the task of identifying the precise boundary of
               every cell in an image. To address this problem we constructed
               TissueNet, a dataset for training segmentation models that
               contains more than 1 million manually labeled cells, an order of
               magnitude more than all previously published segmentation
               training datasets. We used TissueNet to train Mesmer, a
               deep-learning-enabled segmentation algorithm. We demonstrated
               that Mesmer is more accurate than previous methods, generalizes
               to the full diversity of tissue types and imaging platforms in
               TissueNet, and achieves human-level performance. Mesmer enabled
               the automated extraction of key cellular features, such as
               subcellular localization of protein signal, which was challenging
               with previous approaches. We then adapted Mesmer to harness cell
               lineage information in highly multiplexed datasets and used this
               enhanced version to quantify cell morphology changes during human
               gestation. All code, data and models are released as a community
               resource.",
  month     =  apr,
  year      =  2022,
  language  = "en"
}

@ARTICLE{TN3,
  title     = "{DeepCell} Kiosk: scaling deep learning-enabled cellular image
               analysis with Kubernetes",
  author    = "Bannon, Dylan and Moen, Erick and Schwartz, Morgan and Borba,
               Enrico and Kudo, Takamasa and Greenwald, Noah and Vijayakumar,
               Vibha and Chang, Brian and Pao, Edward and Osterman, Erik and
               Graf, William and Van Valen, David",
  journal   = "Nat. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  18,
  number    =  1,
  pages     = "43--45",
  abstract  = "Deep learning is transforming the analysis of biological images,
               but applying these models to large datasets remains challenging.
               Here we describe the DeepCell Kiosk, cloud-native software that
               dynamically scales deep learning workflows to accommodate large
               imaging datasets. To demonstrate the scalability and
               affordability of this software, we identified cell nuclei in 106
               1-megapixel images in ~5.5 h for ~US$250, with a cost below
               US$100 achievable depending on cluster configuration. The
               DeepCell Kiosk can be downloaded at
               https://github.com/vanvalenlab/kiosk-console ; a persistent
               deployment is available at https://deepcell.org/ .",
  month     =  jan,
  year      =  2021,
  language  = "en"
}

@ARTICLE{TN4,
  title       = "Caliban: Accurate cell tracking and lineage construction in
                 live-cell imaging experiments with deep learning",
  author      = "Schwartz, Morgan Sarah and Moen, Erick and Miller, Geneva and
                 Dougherty, Tom and Borba, Enrico and Ding, Rachel and Graf,
                 William and Pao, Edward and Van Valen, David",
  journal     = "bioRxiv",
  institution = "bioRxiv",
  pages       =  803205,
  abstract    = "AbstractWhile live-cell imaging is a powerful approach to
                 studying the dynamics of cellular systems, converting these
                 imaging data into quantitative, single-cell records of cellular
                 behavior has been a longstanding challenge. Deep learning
                 methods have proven capable of performing cell segmentation—a
                 critical task for analyzing live-cell imaging data—but their
                 performance in cell tracking has been limited by a lack of
                 dynamic datasets with temporally consistent single-cell labels.
                 We bridge this gap through the integrated development of
                 labeling and deep learning methodology. We present a new
                 framework for scalable, human-in-the-loop labeling of live-cell
                 imaging movies, which we use to label a large collection of
                 movies of fluorescently labeled cell nuclei. We use these data
                 to create a new deep-learning-based cell-tracking method that
                 achieves state-of-the-art performance in cell tracking. We have
                 made all of the data, code, and software publicly available
                 with permissive open-source licensing through the DeepCell
                 project’s web portalhttps://deepcell.org.",
  month       =  oct,
  year        =  2019,
  language    = "en"
}


@article{stringer2021cellpose,
  title={Cellpose: a generalist algorithm for cellular segmentation},
  author={Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},
  journal={Nature methods},
  volume={18},
  number={1},
  pages={100--106},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@article{mavska2023cell,
  title={The cell tracking challenge: 10 years of objective benchmarking},
  author={Ma{\v{s}}ka, Martin and Ulman, Vladim{\'\i}r and Delgado-Rodriguez, Pablo and G{\'o}mez-de-Mariscal, Estibaliz and Ne{\v{c}}asov{\'a}, Tereza and Guerrero Pe{\~n}a, Fidel A and Ren, Tsang Ing and Meyerowitz, Elliot M and Scherr, Tim and L{\"o}ffler, Katharina and others},
  journal={Nature Methods},
  volume={20},
  number={7},
  pages={1010--1020},
  year={2023},
  publisher={Nature Publishing Group US New York}
}

@article{luo2021multiple,
  title={Multiple object tracking: A literature review},
  author={Luo, Wenhan and Xing, Junliang and Milan, Anton and Zhang, Xiaoqin and Liu, Wei and Kim, Tae-Kyun},
  journal={Artificial intelligence},
  volume={293},
  pages={103448},
  year={2021},
  publisher={Elsevier}
}

@article{chen2024cmtt,
  title={CMTT-JTracker: a fully test-time adaptive framework serving automated cell lineage construction},
  author={Chen, Liuyin and Fu, Sanyuan and Zhang, Zijun},
  journal={Briefings in Bioinformatics},
  volume={25},
  number={6},
  pages={bbae591},
  year={2024},
  publisher={Oxford University Press}
}





@ARTICLE{Ruthberg2025-mp,
  title     = "Neural radiance fields ({NeRF}) for {3D} reconstruction of
               monocular endoscopic video in sinus surgery",
  author    = "Ruthberg, Jeremy S and Bly, Randall and Gunderson, Nicole and
               Chen, Pengcheng and Alighezi, Mahdi and Seibel, Eric J and
               Abuzeid, Waleed M",
  journal   = "Otolaryngol. Head Neck Surg.",
  publisher = "Wiley Online Library",
  month     =  jan,
  year      =  2025,
  keywords  = "3D reconstruction; computer vision; computer-assisted surgery;
               endoscopic sinus surgery; neural radiance fields; skull-base
               surgery",
  language  = "en"
}

@INPROCEEDINGS{Feng2023-km,
  title     = "{3D} Spatial Multimodal Knowledge Accumulation for Scene Graph
               Prediction in Point Cloud",
  author    = "Feng, Mingtao and Hou, Haoran and Zhang, Liang and Wu, Zijie and
               Guo, Yulan and Mian, Ajmal",
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and
               Pattern Recognition",
  pages     = "9182--9191",
  year      =  2023
}

@ARTICLE{Hong2023-ak,
  title         = "Unifying correspondence, pose and {NeRF} for pose-free novel
                   view synthesis from stereo pairs",
  author        = "Hong, Sunghwan and Jung, Jaewoo and Shin, Heeseong and Yang,
                   Jiaolong and Kim, Seungryong and Luo, Chong",
  journal       = "arXiv [cs.CV]",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Tosi2024-fd,
  title         = "How {NeRFs} and {3D} Gaussian Splatting are reshaping {SLAM}:
                   A survey",
  author        = "Tosi, Fabio and Zhang, Youmin and Gong, Ziren and Sandström,
                   Erik and Mattoccia, Stefano and Oswald, Martin R and Poggi,
                   Matteo",
  journal       = "arXiv [cs.CV]",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}


@ARTICLE{Mallick2024-ro,
  title         = "Taming {3DGS}: High-quality radiance fields with limited
                   resources",
  author        = "Mallick, Saswat Subhajyoti and Goel, Rahul and Kerbl,
                   Bernhard and Carrasco, Francisco Vicente and Steinberger,
                   Markus and De La Torre, Fernando",
  journal       = "arXiv [cs.CV]",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Shi2023-cf,
  title         = "{ColonNeRF}: Neural radiance fields for high-fidelity
                   long-sequence colonoscopy reconstruction",
  author        = "Shi, Yufei and Lu, Beijia and Liu, Jia-Wei and Li, Ming and
                   Shou, Mike Zheng",
  journal       = "arXiv [cs.CV]",

  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@INPROCEEDINGS{Liu2023-jz,
  title     = "Robust Dynamic Radiance Fields",
  author    = "Liu, Yu-Lun and Gao, Chen and Meuleman, Andréas and Tseng,
               Hung-Yu and Saraf, Ayush and Kim, Changil and Chuang, Yung-Yu and
               Kopf, Johannes and Huang, Jia-Bin",
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and
               Pattern Recognition",
  pages     = "13--23",
  year      =  2023
}

@INPROCEEDINGS{Guo2025-yr,
  title     = "{GaussianSlicer}: Efficient surface reconstruction from
               cross-sectional slices with Gaussian splatting",
  author    = "Guo, Yuhu and Qian, Chenghao and Mo, Yuhong and Sangpetch,
               Akkarit",
  booktitle = "ICASSP 2025 - 2025 IEEE International Conference on Acoustics,
               Speech and Signal Processing (ICASSP)",
  publisher = "IEEE",
  pages     = "1--5",
  month     =  apr,
  year      =  2025,
  language  = "en"
}

@INPROCEEDINGS{Keetha2024-jt,
  title     = "{SplaTAM}: Splat Track \& Map {3D} Gaussians for Dense {RGB}-{D}
               {SLAM}",
  author    = "Keetha, Nikhil and Karhade, Jay and Jatavallabhula, Krishna
               Murthy and Yang, Gengshan and Scherer, Sebastian and Ramanan,
               Deva and Luiten, Jonathon",
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and
               Pattern Recognition",
  pages     = "21357--21366",
  year      =  2024
}

@inproceedings{wang2022continual,
  title={Continual test-time domain adaptation},
  author={Wang, Qin and Fink, Olga and Van Gool, Luc and Dai, Dengxin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7201--7211},
  year={2022}
}

@ARTICLE{Shin2024-kq,
  title         = "Enhancing temporal consistency in Video Editing by
                   reconstructing videos with {3D} Gaussian Splatting",
  author        = "Shin, Inkyu and Yu, Qihang and Shen, Xiaohui and Kweon, In So
                   and Yoon, Kuk-Jin and Chen, Liang-Chieh",
  journal       = "arXiv [cs.CV]",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@MISC{Yang_undated-pg,
  title       = "Deformable-{3D}-Gaussians: [CVPR {2024]} Official
                 implementation of ``Deformable {3D} Gaussians for High-Fidelity
                 Monocular Dynamic Scene Reconstruction''",
  author      = "Yang, Ziyi",
  institution = "Github",
  abstract    = "[CVPR 2024] Official implementation of ``Deformable 3D
                 Gaussians for High-Fidelity Monocular Dynamic Scene
                 Reconstruction'' - ingra14m/Deformable-3D-Gaussians",
  language    = "en"
}

@ARTICLE{Qian2023-jy,
  title     = "{3DGS}-Avatar: Animatable Avatars via Deformable {3D} Gaussian
               Splatting",
  author    = "Qian, Zhiyin and Wang, Shaofei and Mihajlovic, Marko and Geiger,
               Andreas and Tang, Siyu",
  journal   = "Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.",
  publisher = "openaccess.thecvf.com",
  pages     = "5020--5030",
  month     =  dec,
  year      =  2023
}

@ARTICLE{Bae2024-xu,
  title         = "Per-Gaussian embedding-based deformation for deformable {3D}
                   Gaussian Splatting",
  author        = "Bae, Jeongmin and Kim, Seoha and Yun, Youngsik and Lee,
                   Hahyun and Bang, Gun and Uh, Youngjung",
  journal       = "arXiv [cs.CV]",
  
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Zhou2023-zg,
  title         = "Feature {3DGS}: Supercharging {3D} Gaussian Splatting to
                   enable distilled feature fields",
  author        = "Zhou, Shijie and Chang, Haoran and Jiang, Sicheng and Fan,
                   Zhiwen and Zhu, Zehao and Xu, Dejia and Chari, Pradyumna and
                   You, Suya and Wang, Zhangyang and Kadambi, Achuta",
  journal       = "arXiv [cs.CV]",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude E},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@inproceedings{keaton2023celltranspose,
  title={Celltranspose: Few-shot domain adaptation for cellular instance segmentation},
  author={Keaton, Matthew R and Zaveri, Ram J and Doretto, Gianfranco},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={455--466},
  year={2023}
}