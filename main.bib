@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})





@article{ershov2022trackmate,
  title={TrackMate 7: integrating state-of-the-art segmentation algorithms into tracking pipelines},
  author={Ershov, Dmitry and Phan, Minh-Son and Pylv{\"a}n{\"a}inen, Joanna W and Rigaud, St{\'e}phane U and Le Blanc, Laure and Charles-Orszag, Arthur and Conway, James RW and Laine, Romain F and Roy, Nathan H and Bonazzi, Daria and others},
  journal={Nature methods},
  volume={19},
  number={7},
  pages={829--832},
  year={2022},
  publisher={Nature Publishing Group US New York}
}


@article{bragantini2024ultrack,
  title={Ultrack: pushing the limits of cell tracking across biological scales},
  author={Bragantini, Jord{\~a}o and Theodoro, Ilan and Zhao, Xiang and Huijben, Teun APM and Hirata-Miyasaki, Eduardo and VijayKumar, Shruthi and Balasubramanian, Akilandeswari and Lao, Tiger and Agrawal, Richa and Xiao, Sheng and others},
  journal={bioRxiv},
  year={2024}
}

@article{TissueNet,
    doi = {10.1371/journal.pcbi.1005177},
    author = {Van Valen, David A. AND Kudo, Takamasa AND Lane, Keara M. AND Macklin, Derek N. AND Quach, Nicolas T. AND DeFelice, Mialy M. AND Maayan, Inbal AND Tanouchi, Yu AND Ashley, Euan A. AND Covert, Markus W.},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Deep Learning Automates the Quantitative Analysis of Individual Cells in Live-Cell Imaging Experiments},
    year = {2016},
    month = {11},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pcbi.1005177},
    pages = {1-24},
    abstract = {Live-cell imaging has opened an exciting window into the role cellular heterogeneity plays in dynamic, living systems. A major critical challenge for this class of experiments is the problem of image segmentation, or determining which parts of a microscope image correspond to which individual cells. Current approaches require many hours of manual curation and depend on approaches that are difficult to share between labs. They are also unable to robustly segment the cytoplasms of mammalian cells. Here, we show that deep convolutional neural networks, a supervised machine learning method, can solve this challenge for multiple cell types across the domains of life. We demonstrate that this approach can robustly segment fluorescent images of cell nuclei as well as phase images of the cytoplasms of individual bacterial and mammalian cells from phase contrast images without the need for a fluorescent cytoplasmic marker. These networks also enable the simultaneous segmentation and identification of different mammalian cell types grown in co-culture. A quantitative comparison with prior methods demonstrates that convolutional neural networks have improved accuracy and lead to a significant reduction in curation time. We relay our experience in designing and optimizing deep convolutional neural networks for this task and outline several design rules that we found led to robust performance. We conclude that deep convolutional neural networks are an accurate method that require less curation time, are generalizable to a multiplicity of cell types, from bacteria to mammalian cells, and expand live-cell imaging capabilities to include multi-cell type systems.},
    number = {11},

}

@ARTICLE{TN2,
  title     = "Whole-cell segmentation of tissue images with human-level
               performance using large-scale data annotation and deep learning",
  author    = "Greenwald, Noah F and Miller, Geneva and Moen, Erick and Kong,
               Alex and Kagel, Adam and Dougherty, Thomas and Fullaway,
               Christine Camacho and McIntosh, Brianna J and Leow, Ke Xuan and
               Schwartz, Morgan Sarah and Pavelchek, Cole and Cui, Sunny and
               Camplisson, Isabella and Bar-Tal, Omer and Singh, Jaiveer and
               Fong, Mara and Chaudhry, Gautam and Abraham, Zion and Moseley,
               Jackson and Warshawsky, Shiri and Soon, Erin and Greenbaum,
               Shirley and Risom, Tyler and Hollmann, Travis and Bendall, Sean C
               and Keren, Leeat and Graf, William and Angelo, Michael and Van
               Valen, David",
  journal   = "Nat. Biotechnol.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  40,
  number    =  4,
  pages     = "555--565",
  abstract  = "A principal challenge in the analysis of tissue imaging data is
               cell segmentation-the task of identifying the precise boundary of
               every cell in an image. To address this problem we constructed
               TissueNet, a dataset for training segmentation models that
               contains more than 1 million manually labeled cells, an order of
               magnitude more than all previously published segmentation
               training datasets. We used TissueNet to train Mesmer, a
               deep-learning-enabled segmentation algorithm. We demonstrated
               that Mesmer is more accurate than previous methods, generalizes
               to the full diversity of tissue types and imaging platforms in
               TissueNet, and achieves human-level performance. Mesmer enabled
               the automated extraction of key cellular features, such as
               subcellular localization of protein signal, which was challenging
               with previous approaches. We then adapted Mesmer to harness cell
               lineage information in highly multiplexed datasets and used this
               enhanced version to quantify cell morphology changes during human
               gestation. All code, data and models are released as a community
               resource.",
  month     =  apr,
  year      =  2022,
  language  = "en"
}

@ARTICLE{TN3,
  title     = "{DeepCell} Kiosk: scaling deep learning-enabled cellular image
               analysis with Kubernetes",
  author    = "Bannon, Dylan and Moen, Erick and Schwartz, Morgan and Borba,
               Enrico and Kudo, Takamasa and Greenwald, Noah and Vijayakumar,
               Vibha and Chang, Brian and Pao, Edward and Osterman, Erik and
               Graf, William and Van Valen, David",
  journal   = "Nat. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  18,
  number    =  1,
  pages     = "43--45",
  abstract  = "Deep learning is transforming the analysis of biological images,
               but applying these models to large datasets remains challenging.
               Here we describe the DeepCell Kiosk, cloud-native software that
               dynamically scales deep learning workflows to accommodate large
               imaging datasets. To demonstrate the scalability and
               affordability of this software, we identified cell nuclei in 106
               1-megapixel images in ~5.5 h for ~US$250, with a cost below
               US$100 achievable depending on cluster configuration. The
               DeepCell Kiosk can be downloaded at
               https://github.com/vanvalenlab/kiosk-console ; a persistent
               deployment is available at https://deepcell.org/ .",
  month     =  jan,
  year      =  2021,
  language  = "en"
}

@ARTICLE{TN4,
  title       = "Caliban: Accurate cell tracking and lineage construction in
                 live-cell imaging experiments with deep learning",
  author      = "Schwartz, Morgan Sarah and Moen, Erick and Miller, Geneva and
                 Dougherty, Tom and Borba, Enrico and Ding, Rachel and Graf,
                 William and Pao, Edward and Van Valen, David",
  journal     = "bioRxiv",
  institution = "bioRxiv",
  pages       =  803205,
  abstract    = "AbstractWhile live-cell imaging is a powerful approach to
                 studying the dynamics of cellular systems, converting these
                 imaging data into quantitative, single-cell records of cellular
                 behavior has been a longstanding challenge. Deep learning
                 methods have proven capable of performing cell segmentation—a
                 critical task for analyzing live-cell imaging data—but their
                 performance in cell tracking has been limited by a lack of
                 dynamic datasets with temporally consistent single-cell labels.
                 We bridge this gap through the integrated development of
                 labeling and deep learning methodology. We present a new
                 framework for scalable, human-in-the-loop labeling of live-cell
                 imaging movies, which we use to label a large collection of
                 movies of fluorescently labeled cell nuclei. We use these data
                 to create a new deep-learning-based cell-tracking method that
                 achieves state-of-the-art performance in cell tracking. We have
                 made all of the data, code, and software publicly available
                 with permissive open-source licensing through the DeepCell
                 project’s web portalhttps://deepcell.org.",
  month       =  oct,
  year        =  2019,
  language    = "en"
}


@article{stringer2021cellpose,
  title={Cellpose: a generalist algorithm for cellular segmentation},
  author={Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},
  journal={Nature methods},
  volume={18},
  number={1},
  pages={100--106},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@article{mavska2023cell,
  title={The cell tracking challenge: 10 years of objective benchmarking},
  author={Ma{\v{s}}ka, Martin and Ulman, Vladim{\'\i}r and Delgado-Rodriguez, Pablo and G{\'o}mez-de-Mariscal, Estibaliz and Ne{\v{c}}asov{\'a}, Tereza and Guerrero Pe{\~n}a, Fidel A and Ren, Tsang Ing and Meyerowitz, Elliot M and Scherr, Tim and L{\"o}ffler, Katharina and others},
  journal={Nature Methods},
  volume={20},
  number={7},
  pages={1010--1020},
  year={2023},
  publisher={Nature Publishing Group US New York}
}

@article{luo2021multiple,
  title={Multiple object tracking: A literature review},
  author={Luo, Wenhan and Xing, Junliang and Milan, Anton and Zhang, Xiaoqin and Liu, Wei and Kim, Tae-Kyun},
  journal={Artificial intelligence},
  volume={293},
  pages={103448},
  year={2021},
  publisher={Elsevier}
}

@article{chen2024cmtt,
  title={CMTT-JTracker: a fully test-time adaptive framework serving automated cell lineage construction},
  author={Chen, Liuyin and Fu, Sanyuan and Zhang, Zijun},
  journal={Briefings in Bioinformatics},
  volume={25},
  number={6},
  pages={bbae591},
  year={2024},
  publisher={Oxford University Press}
}





@ARTICLE{Ruthberg2025-mp,
  title     = "Neural radiance fields ({NeRF}) for {3D} reconstruction of
               monocular endoscopic video in sinus surgery",
  author    = "Ruthberg, Jeremy S and Bly, Randall and Gunderson, Nicole and
               Chen, Pengcheng and Alighezi, Mahdi and Seibel, Eric J and
               Abuzeid, Waleed M",
  journal   = "Otolaryngol. Head Neck Surg.",
  publisher = "Wiley Online Library",
  month     =  jan,
  year      =  2025,
  keywords  = "3D reconstruction; computer vision; computer-assisted surgery;
               endoscopic sinus surgery; neural radiance fields; skull-base
               surgery",
  language  = "en"
}

@INPROCEEDINGS{Feng2023-km,
  title     = "{3D} Spatial Multimodal Knowledge Accumulation for Scene Graph
               Prediction in Point Cloud",
  author    = "Feng, Mingtao and Hou, Haoran and Zhang, Liang and Wu, Zijie and
               Guo, Yulan and Mian, Ajmal",
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and
               Pattern Recognition",
  pages     = "9182--9191",
  year      =  2023
}

@ARTICLE{Hong2023-ak,
  title         = "Unifying correspondence, pose and {NeRF} for pose-free novel
                   view synthesis from stereo pairs",
  author        = "Hong, Sunghwan and Jung, Jaewoo and Shin, Heeseong and Yang,
                   Jiaolong and Kim, Seungryong and Luo, Chong",
  journal       = "arXiv [cs.CV]",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Tosi2024-fd,
  title         = "How {NeRFs} and {3D} Gaussian Splatting are reshaping {SLAM}:
                   A survey",
  author        = "Tosi, Fabio and Zhang, Youmin and Gong, Ziren and Sandström,
                   Erik and Mattoccia, Stefano and Oswald, Martin R and Poggi,
                   Matteo",
  journal       = "arXiv [cs.CV]",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}


@ARTICLE{Mallick2024-ro,
  title         = "Taming {3DGS}: High-quality radiance fields with limited
                   resources",
  author        = "Mallick, Saswat Subhajyoti and Goel, Rahul and Kerbl,
                   Bernhard and Carrasco, Francisco Vicente and Steinberger,
                   Markus and De La Torre, Fernando",
  journal       = "arXiv [cs.CV]",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Shi2023-cf,
  title         = "{ColonNeRF}: Neural radiance fields for high-fidelity
                   long-sequence colonoscopy reconstruction",
  author        = "Shi, Yufei and Lu, Beijia and Liu, Jia-Wei and Li, Ming and
                   Shou, Mike Zheng",
  journal       = "arXiv [cs.CV]",

  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@INPROCEEDINGS{Liu2023-jz,
  title     = "Robust Dynamic Radiance Fields",
  author    = "Liu, Yu-Lun and Gao, Chen and Meuleman, Andréas and Tseng,
               Hung-Yu and Saraf, Ayush and Kim, Changil and Chuang, Yung-Yu and
               Kopf, Johannes and Huang, Jia-Bin",
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and
               Pattern Recognition",
  pages     = "13--23",
  year      =  2023
}

@INPROCEEDINGS{Guo2025-yr,
  title     = "{GaussianSlicer}: Efficient surface reconstruction from
               cross-sectional slices with Gaussian splatting",
  author    = "Guo, Yuhu and Qian, Chenghao and Mo, Yuhong and Sangpetch,
               Akkarit",
  booktitle = "ICASSP 2025 - 2025 IEEE International Conference on Acoustics,
               Speech and Signal Processing (ICASSP)",
  publisher = "IEEE",
  pages     = "1--5",
  month     =  apr,
  year      =  2025,
  language  = "en"
}

@INPROCEEDINGS{Keetha2024-jt,
  title     = "{SplaTAM}: Splat Track \& Map {3D} Gaussians for Dense {RGB}-{D}
               {SLAM}",
  author    = "Keetha, Nikhil and Karhade, Jay and Jatavallabhula, Krishna
               Murthy and Yang, Gengshan and Scherer, Sebastian and Ramanan,
               Deva and Luiten, Jonathon",
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and
               Pattern Recognition",
  pages     = "21357--21366",
  year      =  2024
}

@inproceedings{wang2022continual,
  title={Continual test-time domain adaptation},
  author={Wang, Qin and Fink, Olga and Van Gool, Luc and Dai, Dengxin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7201--7211},
  year={2022}
}

@ARTICLE{Shin2024-kq,
  title         = "Enhancing temporal consistency in Video Editing by
                   reconstructing videos with {3D} Gaussian Splatting",
  author        = "Shin, Inkyu and Yu, Qihang and Shen, Xiaohui and Kweon, In So
                   and Yoon, Kuk-Jin and Chen, Liang-Chieh",
  journal       = "arXiv [cs.CV]",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@MISC{Yang_undated-pg,
  title       = "Deformable-{3D}-Gaussians: [CVPR {2024]} Official
                 implementation of ``Deformable {3D} Gaussians for High-Fidelity
                 Monocular Dynamic Scene Reconstruction''",
  author      = "Yang, Ziyi",
  institution = "Github",
  abstract    = "[CVPR 2024] Official implementation of ``Deformable 3D
                 Gaussians for High-Fidelity Monocular Dynamic Scene
                 Reconstruction'' - ingra14m/Deformable-3D-Gaussians",
  language    = "en"
}

@ARTICLE{Qian2023-jy,
  title     = "{3DGS}-Avatar: Animatable Avatars via Deformable {3D} Gaussian
               Splatting",
  author    = "Qian, Zhiyin and Wang, Shaofei and Mihajlovic, Marko and Geiger,
               Andreas and Tang, Siyu",
  journal   = "Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.",
  publisher = "openaccess.thecvf.com",
  pages     = "5020--5030",
  month     =  dec,
  year      =  2023
}

@ARTICLE{Bae2024-xu,
  title         = "Per-Gaussian embedding-based deformation for deformable {3D}
                   Gaussian Splatting",
  author        = "Bae, Jeongmin and Kim, Seoha and Yun, Youngsik and Lee,
                   Hahyun and Bang, Gun and Uh, Youngjung",
  journal       = "arXiv [cs.CV]",
  
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Zhou2023-zg,
  title         = "Feature {3DGS}: Supercharging {3D} Gaussian Splatting to
                   enable distilled feature fields",
  author        = "Zhou, Shijie and Chang, Haoran and Jiang, Sicheng and Fan,
                   Zhiwen and Zhu, Zehao and Xu, Dejia and Chari, Pradyumna and
                   You, Suya and Wang, Zhangyang and Kadambi, Achuta",
  journal       = "arXiv [cs.CV]",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude E},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@inproceedings{keaton2023celltranspose,
  title={Celltranspose: Few-shot domain adaptation for cellular instance segmentation},
  author={Keaton, Matthew R and Zaveri, Ram J and Doretto, Gianfranco},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={455--466},
  year={2023}
}

@inproceedings{wang2021understanding,
  title={Understanding the behaviour of contrastive loss},
  author={Wang, Feng and Liu, Huaping},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2495--2504},
  year={2021}
}

@ARTICLE{Chen2022-vt,
  title         = "Contrastive Test-Time Adaptation",
  author        = "Chen, Dian and Wang, Dequan and Darrell, Trevor and Ebrahimi,
                   Sayna",
  journal       = "arXiv [cs.CV]",
  abstract      = "Test-time adaptation is a special setting of unsupervised
                   domain adaptation where a trained model on the source domain
                   has to adapt to the target domain without accessing source
                   data. We propose a novel way to leverage self-supervised
                   contrastive learning to facilitate target feature learning,
                   along with an online pseudo labeling scheme with refinement
                   that significantly denoises pseudo labels. The contrastive
                   learning task is applied jointly with pseudo labeling,
                   contrasting positive and negative pairs constructed similarly
                   as MoCo but with source-initialized encoder, and excluding
                   same-class negative pairs indicated by pseudo labels.
                   Meanwhile, we produce pseudo labels online and refine them
                   via soft voting among their nearest neighbors in the target
                   feature space, enabled by maintaining a memory queue. Our
                   method, AdaContrast, achieves state-of-the-art performance on
                   major benchmarks while having several desirable properties
                   compared to existing works, including memory efficiency,
                   insensitivity to hyper-parameters, and better model
                   calibration. Project page: sites.google.com/view/adacontrast.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}


@ARTICLE{Liang2024-je,
  title     = "A comprehensive survey on test-time adaptation under distribution
               shifts",
  author    = "Liang, Jian and He, Ran and Tan, Tieniu",
  journal   = "Int. J. Comput. Vis.",
  publisher = "Springer Science and Business Media LLC",
  month     =  jul,
  year      =  2024,
  language  = "en"
}


@ARTICLE{He2021-wn,
  title     = "Autoencoder based self-supervised test-time adaptation for
               medical image analysis",
  author    = "He, Yufan and Carass, Aaron and Zuo, Lianrui and Dewey, Blake E
               and Prince, Jerry L",
  journal   = "Med. Image Anal.",
  publisher = "Elsevier BV",
  volume    =  72,
  number    =  102136,
  pages     =  102136,
  abstract  = "Deep neural networks have been successfully applied to medical
               image analysis tasks like segmentation and synthesis. However,
               even if a network is trained on a large dataset from the source
               domain, its performance on unseen test domains is not guaranteed.
               The performance drop on data obtained differently from the
               network's training data is a major problem (known as domain
               shift) in deploying deep learning in clinical practice. Existing
               work focuses on retraining the model with data from the test
               domain, or harmonizing the test domain's data to the network
               training data. A common practice is to distribute a
               carefully-trained model to multiple users (e.g., clinical
               centers), and then each user uses the model to process their own
               data, which may have a domain shift (e.g., varying imaging
               parameters and machines). However, the lack of availability of
               the source training data and the cost of training a new model
               often prevents the use of known methods to solve user-specific
               domain shifts. Here, we ask whether we can design a model that,
               once distributed to users, can quickly adapt itself to each new
               site without expensive retraining or access to the source
               training data? In this paper, we propose a model that can adapt
               based on a single test subject during inference. The model
               consists of three parts, which are all neural networks: a task
               model (T) which performs the image analysis task like
               segmentation; a set of autoencoders (AEs); and a set of adaptors
               (As). The task model and autoencoders are trained on the source
               dataset and can be computationally expensive. In the deployment
               stage, the adaptors are trained to transform the test image and
               its features to minimize the domain shift as measured by the
               autoencoders' reconstruction loss. Only the adaptors are
               optimized during the testing stage with a single test subject
               thus is computationally efficient. The method was validated on
               both retinal optical coherence tomography (OCT) image
               segmentation and magnetic resonance imaging (MRI) T1-weighted to
               T2-weighted image synthesis. Our method, with its short
               optimization time for the adaptors (10 iterations on a single
               test subject) and its additional required disk space for the
               autoencoders (around 15 MB), can achieve significant performance
               improvement. Our code is publicly available at:
               https://github.com/YufanHe/self-domain-adapted-network.",
  month     =  aug,
  year      =  2021,
  keywords  = "Medical image analysis; Self supervised learning; Test time
               adaptation; Unsupervised domain adaptation",
  language  = "en"
}

@ARTICLE{Valanarasu2022-zo,
  title         = "On-the-fly test-time adaptation for medical image
                   segmentation",
  author        = "Valanarasu, Jeya Maria Jose and Guo, Pengfei and Vs, Vibashan
                   and Patel, Vishal M",
  journal       = "arXiv [eess.IV]",
  abstract      = "One major problem in deep learning-based solutions for
                   medical imaging is the drop in performance when a model is
                   tested on a data distribution different from the one that it
                   is trained on. Adapting the source model to target data
                   distribution at test-time is an efficient solution for the
                   data-shift problem. Previous methods solve this by adapting
                   the model to target distribution by using techniques like
                   entropy minimization or regularization. In these methods, the
                   models are still updated by back-propagation using an
                   unsupervised loss on complete test data distribution. In
                   real-world clinical settings, it makes more sense to adapt a
                   model to a new test image on-the-fly and avoid model update
                   during inference due to privacy concerns and lack of
                   computing resource at deployment. To this end, we propose a
                   new setting - On-the-Fly Adaptation which is zero-shot and
                   episodic (i.e., the model is adapted to a single image at a
                   time and also does not perform any back-propagation during
                   test-time). To achieve this, we propose a new framework
                   called Adaptive UNet where each convolutional block is
                   equipped with an adaptive batch normalization layer to adapt
                   the features with respect to a domain code. The domain code
                   is generated using a pre-trained encoder trained on a large
                   corpus of medical images. During test-time, the model takes
                   in just the new test image and generates a domain code to
                   adapt the features of source model according to the test
                   data. We validate the performance on both 2D and 3D data
                   distribution shifts where we get a better performance
                   compared to previous test-time adaptation methods. Code is
                   available at
                   https://github.com/jeya-maria-jose/On-The-Fly-Adaptation",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV"
}

@ARTICLE{Karani2021-sz,
  title     = "Test-time adaptable neural networks for robust medical image
               segmentation",
  author    = "Karani, Neerav and Erdil, Ertunc and Chaitanya, Krishna and
               Konukoglu, Ender",
  journal   = "Med. Image Anal.",
  publisher = "Elsevier BV",
  volume    =  68,
  number    =  101907,
  pages     =  101907,
  abstract  = "Convolutional Neural Networks (CNNs) work very well for
               supervised learning problems when the training dataset is
               representative of the variations expected to be encountered at
               test time. In medical image segmentation, this premise is
               violated when there is a mismatch between training and test
               images in terms of their acquisition details, such as the scanner
               model or the protocol. Remarkable performance degradation of CNNs
               in this scenario is well documented in the literature. To address
               this problem, we design the segmentation CNN as a concatenation
               of two sub-networks: a relatively shallow image normalization
               CNN, followed by a deep CNN that segments the normalized image.
               We train both these sub-networks using a training dataset,
               consisting of annotated images from a particular scanner and
               protocol setting. Now, at test time, we adapt the image
               normalization sub-network for each test image, guided by an
               implicit prior on the predicted segmentation labels. We employ an
               independently trained denoising autoencoder (DAE) in order to
               model such an implicit prior on plausible anatomical segmentation
               labels. We validate the proposed idea on multi-center Magnetic
               Resonance imaging datasets of three anatomies: brain, heart and
               prostate. The proposed test-time adaptation consistently provides
               performance improvement, demonstrating the promise and generality
               of the approach. Being agnostic to the architecture of the deep
               CNN, the second sub-network, the proposed design can be utilized
               with any segmentation network to increase robustness to
               variations in imaging scanners and protocols. Our code is
               available at:
               https://github.com/neerakara/test-time-adaptable-neural-networks-for-domain-generalization.",
  month     =  feb,
  year      =  2021,
  keywords  = "Cross-protocol robustness; Cross-scanner robustness; Domain
               generalization; Medical image segmentation",
  language  = "en"
}

@ARTICLE{Liang2024-je,
  title     = "A comprehensive survey on test-time adaptation under distribution
               shifts",
  author    = "Liang, Jian and He, Ran and Tan, Tieniu",
  journal   = "Int. J. Comput. Vis.",
  publisher = "Springer Science and Business Media LLC",
  month     =  jul,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Chen2022-vt,
  title         = "Contrastive Test-Time Adaptation",
  author        = "Chen, Dian and Wang, Dequan and Darrell, Trevor and Ebrahimi,
                   Sayna",
  journal       = "arXiv [cs.CV]",
  abstract      = "Test-time adaptation is a special setting of unsupervised
                   domain adaptation where a trained model on the source domain
                   has to adapt to the target domain without accessing source
                   data. We propose a novel way to leverage self-supervised
                   contrastive learning to facilitate target feature learning,
                   along with an online pseudo labeling scheme with refinement
                   that significantly denoises pseudo labels. The contrastive
                   learning task is applied jointly with pseudo labeling,
                   contrasting positive and negative pairs constructed similarly
                   as MoCo but with source-initialized encoder, and excluding
                   same-class negative pairs indicated by pseudo labels.
                   Meanwhile, we produce pseudo labels online and refine them
                   via soft voting among their nearest neighbors in the target
                   feature space, enabled by maintaining a memory queue. Our
                   method, AdaContrast, achieves state-of-the-art performance on
                   major benchmarks while having several desirable properties
                   compared to existing works, including memory efficiency,
                   insensitivity to hyper-parameters, and better model
                   calibration. Project page: sites.google.com/view/adacontrast.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Bannon2021-qv,
  title     = "{DeepCell} Kiosk: scaling deep learning-enabled cellular image
               analysis with Kubernetes",
  author    = "Bannon, Dylan and Moen, Erick and Schwartz, Morgan and Borba,
               Enrico and Kudo, Takamasa and Greenwald, Noah and Vijayakumar,
               Vibha and Chang, Brian and Pao, Edward and Osterman, Erik and
               Graf, William and Van Valen, David",
  journal   = "Nat. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  18,
  number    =  1,
  pages     = "43--45",
  abstract  = "Deep learning is transforming the analysis of biological images,
               but applying these models to large datasets remains challenging.
               Here we describe the DeepCell Kiosk, cloud-native software that
               dynamically scales deep learning workflows to accommodate large
               imaging datasets. To demonstrate the scalability and
               affordability of this software, we identified cell nuclei in 106
               1-megapixel images in ~5.5 h for ~US$250, with a cost below
               US$100 achievable depending on cluster configuration. The
               DeepCell Kiosk can be downloaded at
               https://github.com/vanvalenlab/kiosk-console ; a persistent
               deployment is available at https://deepcell.org/ .",
  month     =  jan,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Schwartz2019-kd,
  title       = "Caliban: Accurate cell tracking and lineage construction in
                 live-cell imaging experiments with deep learning",
  author      = "Schwartz, Morgan Sarah and Moen, Erick and Miller, Geneva and
                 Dougherty, Tom and Borba, Enrico and Ding, Rachel and Graf,
                 William and Pao, Edward and Van Valen, David",
  journal     = "bioRxiv",
  institution = "bioRxiv",
  pages       =  803205,
  abstract    = "AbstractWhile live-cell imaging is a powerful approach to
                 studying the dynamics of cellular systems, converting these
                 imaging data into quantitative, single-cell records of cellular
                 behavior has been a longstanding challenge. Deep learning
                 methods have proven capable of performing cell segmentation—a
                 critical task for analyzing live-cell imaging data—but their
                 performance in cell tracking has been limited by a lack of
                 dynamic datasets with temporally consistent single-cell labels.
                 We bridge this gap through the integrated development of
                 labeling and deep learning methodology. We present a new
                 framework for scalable, human-in-the-loop labeling of live-cell
                 imaging movies, which we use to label a large collection of
                 movies of fluorescently labeled cell nuclei. We use these data
                 to create a new deep-learning-based cell-tracking method that
                 achieves state-of-the-art performance in cell tracking. We have
                 made all of the data, code, and software publicly available
                 with permissive open-source licensing through the DeepCell
                 project’s web portalhttps://deepcell.org.",
  month       =  oct,
  year        =  2019,
  language    = "en"
}

@ARTICLE{Alnaggar2024-wm,
  title     = "Efficient artificial intelligence approaches for medical image
               processing in healthcare: comprehensive review, taxonomy, and
               analysis",
  author    = "Alnaggar, Omar Abdullah Murshed Farhan and Jagadale, Basavaraj N
               and Saif, Mufeed Ahmed Naji and Ghaleb, Osamah A M and Ahmed,
               Ammar A Q and Aqlan, Hesham Abdo Ahmed and Al-Ariki, Hasib Daowd
               Esmail",
  journal   = "Artif. Intell. Rev.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  57,
  number    =  8,
  pages     = "1--139",
  abstract  = "AbstractIn healthcare, medical practitioners employ various
               imaging techniques such as CT, X-ray, PET, and MRI to diagnose
               patients, emphasizing the crucial need for early disease
               detection to enhance survival rates. Medical Image Analysis (MIA)
               has undergone a transformative shift with the integration of
               Artificial Intelligence (AI) techniques such as Machine Learning
               (ML) and Deep Learning (DL), promising advanced diagnostics and
               improved healthcare outcomes. Despite these advancements, a
               comprehensive understanding of the efficiency metrics,
               computational complexities, interpretability, and scalability of
               AI based approaches in MIA is essential for practical feasibility
               in real-world healthcare environments. Existing studies exploring
               AI applications in MIA lack a consolidated review covering the
               major MIA stages and specifically focused on evaluating the
               efficiency of AI based approaches. The absence of a structured
               framework limits decision-making for researchers, practitioners,
               and policymakers in selecting and implementing optimal AI
               approaches in healthcare. Furthermore, the lack of standardized
               evaluation metrics complicates methodology comparison, hindering
               the development of efficient approaches. This article addresses
               these challenges through a comprehensive review, taxonomy, and
               analysis of existing AI-based MIA approaches in healthcare. The
               taxonomy covers major image processing stages, classifying AI
               approaches for each stage based on method and further analyzing
               them based on image origin, objective, method, dataset, and
               evaluation metrics to reveal their strengths and weaknesses.
               Additionally, comparative analysis conducted to evaluate the
               efficiency of AI based MIA approaches over five publically
               available datasets: ISIC 2018, CVC-Clinic, 2018 DSB, DRIVE, and
               EM in terms of accuracy, precision, Recall, F-measure, mIoU, and
               specificity. The popular public datasets and evaluation metrics
               are briefly described and analyzed. The resulting taxonomy
               provides a structured framework for understanding the AI
               landscape in healthcare, facilitating evidence-based
               decision-making and guiding future research efforts toward the
               development of efficient and scalable AI approaches to meet
               current healthcare needs.",
  month     =  jul,
  year      =  2024,
  language  = "en"
}
