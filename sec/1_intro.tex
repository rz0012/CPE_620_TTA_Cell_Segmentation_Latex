\section{Introduction}
\label{sec:intro}

% Why are we tracking cells in the first place (Cellular Activity Monitoring)

Monitoring cells and their activities underpin experiments across the biological sciences.  Various types of microscopes can be employed to produce videos or time-sequence observations of 3d volumes, allowing them to collect information about how cells interact with each other and external stimuli.  This can look like light-sheet, fluorescent, or confocal microscopy, each requiring tracking the cells in the volume they occupy; fully automated tracking of sufficient quality at scale has not yet been perfected~\cite{mavska2023cell}.\\

The first candidate for direct application from the rest of the computer vision community would be solutions to multi-object tracking.  In multiobject tracking, the archetypal framing of the problem is one of camera surveillance over people in a volume.  The surveillance camera has to deal with occlusion when one person passes in front of the other or collision when two people interact before going their separate ways~\cite{luo2021multiple}.  Cells exhibit two specific behaviors of interest, which make them act, unlike a crowd of people, specifically mitosis, the process by which one cell divides into two identical daughter cells and apoptosis, the process of cell death. Another major challenge is that almost all multi-object tracking frameworks are appearance-based, learning features that maximize similarity among the same instance across different time steps. This procedure renders less valuable in cell tracking as most cells do not possess such rich appearance distinctiveness~\cite{mavska2023cell}. \\

Researchers in biomedical sciences have taken inspiration from the computer vision community and predominantly used tracking by detection paradigms, where first, the detection or instance segmentation is performed to give individual objects a distinct ID~\cite{bragantini2024ultrack,ershov2022trackmate}. However, most algorithms utilize IOU or Distance-based matching instead of direct appearance-based matching. These values are used as weights in the association matrix to construct a graph. An operation similar to beam search is performed to prune the paths with a lower cost with respect to a predefined threshold.  An integer linear program is used to find the best paths for each initial node in the graph. Constraints are put on the integer linear program to accommodate mitosis (cell division) and cell death. Generally, that is how cell tracking is done~\cite{mavska2023cell}. \\

Some approaches focus on improving the detection paradigm, while others focus on improving their association matrix for the integer linear program~\cite{mavska2023cell}. However, all of these approaches are limited by the datasets they use and are tailored to those. Minimal work has been done to accommodate datasets from various sources. As mentioned previously, these datasets can be collected from many different imaging modalities. Cells can also possess different sizes, shapes, and dynamics. One tracking approach cannot accommodate all of these challenges~\cite{chen2024cmtt}. Therefore, there has to be an adaptive framework that can quickly adapt to a new dataset. However, the adaptation has to be unsupervised and efficient to add real-world constraints. Therefore, we propose to tackle the test-time adaptation problem of cell instance segmentation to improve cell tracking.\\

We hypothesize that just performing test time adaptation on cell segmentation will significantly improve cell tracking performance since the associations are based on simple metrics such as IoU and Euclidian distance of the centroids. We propose to pretrain a denoising framework that takes three inputs: the raw image, noisy segmentation from a pretrained segmentation model, and Gaussian splats retrieved from the raw image, and produce a clean segmentation mask as pseudo labels. Once the pseudo labels are retrieved, we perform knowledge distillation with stochastic restoration of parameters to perform the test-time adaptation. More details are in the methods.   
